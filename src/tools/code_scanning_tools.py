#!/usr/bin/env python3
"""
AutoGenæ¡†æ¶ä¸‹çš„ä»£ç æ‰«æå·¥å…·

æä¾›ç‹¬ç«‹çš„Pythonå‡½æ•°ä½œä¸ºAutoGen Agentçš„å·¥å…·ï¼Œç”¨äºä»£ç è´¨é‡åˆ†æã€‚
"""

import asyncio
import json
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# å¯¼å…¥åˆ†æå·¥å…·
try:
    import radon.complexity as radon_cc
    import radon.metrics as radon_metrics
    RADON_AVAILABLE = True
except ImportError:
    RADON_AVAILABLE = False
    logging.warning("radonæœªå®‰è£…ï¼Œå¤æ‚åº¦åˆ†æåŠŸèƒ½å°†å—é™")

logger = logging.getLogger(__name__)


async def scan_code(
    path: str, 
    scan_types: Optional[List[str]] = None,
    output_format: str = "markdown"
) -> str:
    """
    æ‰«ææŒ‡å®šè·¯å¾„çš„Pythonä»£ç å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š
    
    Args:
        path: è¦æ‰«æçš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
        scan_types: æ‰«æç±»å‹åˆ—è¡¨ï¼Œå¯é€‰å€¼ï¼šcomplexity, style, security, documentation, cleanup
        output_format: è¾“å‡ºæ ¼å¼ï¼Œæ”¯æŒ markdown æˆ– json
    
    Returns:
        æ‰«ææŠ¥å‘Šå†…å®¹
    """
    try:
        if scan_types is None:
            scan_types = ["complexity", "style", "security", "documentation", "cleanup"]
        
        target_path = Path(path)
        if not target_path.exists():
            return f"é”™è¯¯ï¼šè·¯å¾„ '{path}' ä¸å­˜åœ¨"
        
        logger.info(f"å¼€å§‹æ‰«æè·¯å¾„: {path}, æ‰«æç±»å‹: {scan_types}")
        
        # æ”¶é›†Pythonæ–‡ä»¶
        python_files = _collect_python_files(target_path)
        if not python_files:
            return "æœªæ‰¾åˆ°Pythonæ–‡ä»¶"
        
        # æ‰§è¡Œåˆ†æ
        analysis_results = {
            "scan_info": {
                "path": str(target_path),
                "scan_types": scan_types,
                "timestamp": datetime.now().isoformat(),
                "files_count": len(python_files)
            },
            "files_analyzed": [str(f) for f in python_files],
            "details": {}
        }
        
        # æ‰§è¡Œå„ç§ç±»å‹çš„åˆ†æ
        for scan_type in scan_types:
            if scan_type == "complexity":
                analysis_results["details"]["complexity"] = await _analyze_complexity(python_files)
            elif scan_type == "style":
                analysis_results["details"]["style"] = await _analyze_style(python_files)
            elif scan_type == "security":
                analysis_results["details"]["security"] = await _analyze_security(python_files)
            elif scan_type == "documentation":
                analysis_results["details"]["documentation"] = await _analyze_documentation(python_files)
            elif scan_type == "cleanup":
                analysis_results["details"]["cleanup"] = await _analyze_cleanup(python_files)
        
        # ç”ŸæˆæŠ¥å‘Š
        if output_format.lower() == "json":
            return json.dumps(analysis_results, indent=2, ensure_ascii=False)
        else:
            return _generate_markdown_report(analysis_results)
            
    except Exception as e:
        logger.error(f"ä»£ç æ‰«æå¤±è´¥: {e}")
        return f"æ‰«æå¤±è´¥: {str(e)}"


async def save_scan_report(
    report_content: str,
    output_path: str,
    format: str = "markdown"
) -> str:
    """
    ä¿å­˜æ‰«ææŠ¥å‘Šåˆ°æ–‡ä»¶
    
    Args:
        report_content: æŠ¥å‘Šå†…å®¹
        output_path: ä¿å­˜è·¯å¾„
        format: æ–‡ä»¶æ ¼å¼
    
    Returns:
        ä¿å­˜ç»“æœä¿¡æ¯
    """
    try:
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")
        return f"æŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {output_path}"
        
    except Exception as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        return f"ä¿å­˜å¤±è´¥: {str(e)}"


async def get_scan_config() -> str:
    """
    è·å–ä»£ç æ‰«æé…ç½®ä¿¡æ¯
    
    Returns:
        é…ç½®ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    config = {
        "available_scan_types": [
            {
                "name": "complexity",
                "description": "å¤æ‚åº¦åˆ†æï¼šåœˆå¤æ‚åº¦ã€è®¤çŸ¥å¤æ‚åº¦ã€Halsteadå¤æ‚åº¦"
            },
            {
                "name": "style",
                "description": "ä»£ç é£æ ¼æ£€æŸ¥ï¼šPEP8åˆè§„æ€§ã€å‘½åè§„èŒƒã€å¯¼å…¥æ’åº"
            },
            {
                "name": "security",
                "description": "å®‰å…¨æ‰«æï¼šæ½œåœ¨å®‰å…¨æ¼æ´ã€å±é™©å‡½æ•°è°ƒç”¨"
            },
            {
                "name": "documentation",
                "description": "æ–‡æ¡£è´¨é‡ï¼šæ–‡æ¡£å­—ç¬¦ä¸²è¦†ç›–ç‡ã€æ³¨é‡Šè´¨é‡"
            },
            {
                "name": "cleanup",
                "description": "ä»£ç æ¸…ç†ï¼šæ­»ä»£ç æ£€æµ‹ã€æœªä½¿ç”¨å¯¼å…¥ã€æ ¼å¼åŒ–å»ºè®®"
            }
        ],
        "supported_formats": ["markdown", "json", "html"],
        "supported_extensions": [".py"],
        "tools_status": {
            "radon": RADON_AVAILABLE,
            "flake8": False,  # å°†åœ¨è¿è¡Œæ—¶æ£€æŸ¥
            "bandit": False,  # å°†åœ¨è¿è¡Œæ—¶æ£€æŸ¥
            "vulture": False  # å°†åœ¨è¿è¡Œæ—¶æ£€æŸ¥
        }
    }
    
    return json.dumps(config, indent=2, ensure_ascii=False)


def _collect_python_files(path: Path) -> List[Path]:
    """æ”¶é›†Pythonæ–‡ä»¶"""
    python_files = []
    supported_extensions = {'.py'}
    
    if path.is_file():
        if path.suffix in supported_extensions:
            python_files.append(path)
    elif path.is_dir():
        for file_path in path.rglob("*"):
            if file_path.is_file() and file_path.suffix in supported_extensions:
                # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
                if any(part in str(file_path) for part in ['.venv', '__pycache__', '.git', 'node_modules']):
                    continue
                python_files.append(file_path)
    
    return python_files


async def _analyze_complexity(files: List[Path]) -> Dict[str, Any]:
    """åˆ†æä»£ç å¤æ‚åº¦"""
    complexity_results = {
        "cyclomatic_complexity": {},
        "summary": {
            "total_functions": 0,
            "high_complexity_functions": [],
            "average_complexity": 0.0
        }
    }
    
    if not RADON_AVAILABLE:
        complexity_results["error"] = "radonå·¥å…·ä¸å¯ç”¨ï¼Œè·³è¿‡å¤æ‚åº¦åˆ†æ"
        return complexity_results
    
    total_complexity = 0
    function_count = 0
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åœˆå¤æ‚åº¦åˆ†æ
            cc_results = radon_cc.cc_visit(content)
            complexity_results["cyclomatic_complexity"][str(file_path)] = []
            
            for result in cc_results:
                complexity_data = {
                    "name": result.name,
                    "type": getattr(result, 'type', 'function'),
                    "complexity": result.complexity,
                    "lineno": result.lineno,
                    "endline": getattr(result, 'endline', result.lineno)
                }
                complexity_results["cyclomatic_complexity"][str(file_path)].append(complexity_data)
                
                function_count += 1
                total_complexity += result.complexity
                
                # æ ‡è®°é«˜å¤æ‚åº¦å‡½æ•°
                if result.complexity > 10:
                    complexity_results["summary"]["high_complexity_functions"].append({
                        "file": str(file_path),
                        "function": result.name,
                        "complexity": result.complexity
                    })
                    
        except Exception as e:
            logger.error(f"å¤æ‚åº¦åˆ†æå¤±è´¥ {file_path}: {e}")
    
    if function_count > 0:
        complexity_results["summary"]["average_complexity"] = round(total_complexity / function_count, 2)
    complexity_results["summary"]["total_functions"] = function_count
    
    return complexity_results


async def _analyze_style(files: List[Path]) -> Dict[str, Any]:
    """åˆ†æä»£ç é£æ ¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–å¤–éƒ¨å·¥å…·ï¼‰"""
    style_results = {
        "basic_checks": {},
        "summary": {
            "total_issues": 0,
            "files_with_issues": 0
        }
    }
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            file_issues = []
            
            # åŸºæœ¬çš„é£æ ¼æ£€æŸ¥
            for i, line in enumerate(lines, 1):
                # æ£€æŸ¥è¡Œé•¿åº¦
                if len(line.rstrip()) > 88:
                    file_issues.append({
                        "line": i,
                        "type": "line_too_long",
                        "message": f"è¡Œé•¿åº¦è¶…è¿‡88å­—ç¬¦ ({len(line.rstrip())})"
                    })
                
                # æ£€æŸ¥å°¾éšç©ºæ ¼
                if line.rstrip() != line.rstrip(' \t'):
                    file_issues.append({
                        "line": i,
                        "type": "trailing_whitespace",
                        "message": "è¡Œæœ«æœ‰å¤šä½™ç©ºæ ¼"
                    })
            
            if file_issues:
                style_results["basic_checks"][str(file_path)] = file_issues
                style_results["summary"]["files_with_issues"] += 1
                style_results["summary"]["total_issues"] += len(file_issues)
                
        except Exception as e:
            logger.error(f"é£æ ¼æ£€æŸ¥å¤±è´¥ {file_path}: {e}")
    
    return style_results


async def _analyze_security(files: List[Path]) -> Dict[str, Any]:
    """åˆ†æå®‰å…¨é—®é¢˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    security_results = {
        "basic_security_checks": {},
        "summary": {
            "total_issues": 0,
            "high_severity": 0,
            "medium_severity": 0,
            "low_severity": 0
        }
    }
    
    # ç®€å•çš„å®‰å…¨æ¨¡å¼æ£€æŸ¥
    security_patterns = [
        ("password", "hardcoded_password", "medium"),
        ("subprocess.call", "dangerous_subprocess", "high"),
        ("eval(", "dangerous_eval", "high"),
        ("exec(", "dangerous_exec", "high"),
        ("shell=True", "shell_injection", "high")
    ]
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.splitlines()
            
            file_issues = []
            
            for i, line in enumerate(lines, 1):
                for pattern, issue_type, severity in security_patterns:
                    if pattern in line.lower():
                        file_issues.append({
                            "line": i,
                            "type": issue_type,
                            "severity": severity,
                            "message": f"å‘ç°æ½œåœ¨å®‰å…¨é—®é¢˜: {pattern}"
                        })
                        security_results["summary"]["total_issues"] += 1
                        security_results["summary"][f"{severity}_severity"] += 1
            
            if file_issues:
                security_results["basic_security_checks"][str(file_path)] = file_issues
                
        except Exception as e:
            logger.error(f"å®‰å…¨æ£€æŸ¥å¤±è´¥ {file_path}: {e}")
    
    return security_results


async def _analyze_documentation(files: List[Path]) -> Dict[str, Any]:
    """åˆ†ææ–‡æ¡£è´¨é‡"""
    doc_results = {
        "docstring_coverage": {},
        "summary": {
            "total_functions": 0,
            "documented_functions": 0,
            "coverage_percentage": 0.0
        }
    }
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„å‡½æ•°å’Œæ–‡æ¡£å­—ç¬¦ä¸²æ£€æµ‹
            import ast
            tree = ast.parse(content)
            
            functions = []
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    has_docstring = (
                        node.body and 
                        isinstance(node.body[0], ast.Expr) and 
                        isinstance(node.body[0].value, ast.Constant) and 
                        isinstance(node.body[0].value.value, str)
                    )
                    
                    functions.append({
                        "name": node.name,
                        "line": node.lineno,
                        "has_docstring": has_docstring
                    })
                    
                    doc_results["summary"]["total_functions"] += 1
                    if has_docstring:
                        doc_results["summary"]["documented_functions"] += 1
            
            if functions:
                doc_results["docstring_coverage"][str(file_path)] = functions
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥ {file_path}: {e}")
    
    # è®¡ç®—è¦†ç›–ç‡
    if doc_results["summary"]["total_functions"] > 0:
        doc_results["summary"]["coverage_percentage"] = round(
            (doc_results["summary"]["documented_functions"] / doc_results["summary"]["total_functions"]) * 100, 2
        )
    
    return doc_results


async def _analyze_cleanup(files: List[Path]) -> Dict[str, Any]:
    """åˆ†æä»£ç æ¸…ç†å»ºè®®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    cleanup_results = {
        "unused_imports": {},
        "summary": {
            "total_unused_imports": 0
        }
    }
    
    for file_path in files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„æœªä½¿ç”¨å¯¼å…¥æ£€æµ‹
            import ast
            tree = ast.parse(content)
            
            imports = []
            used_names = set()
            
            # æ”¶é›†å¯¼å…¥
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.Name):
                    used_names.add(node.id)
            
            # æ£€æŸ¥æœªä½¿ç”¨çš„å¯¼å…¥
            unused = [imp for imp in imports if imp not in used_names]
            
            if unused:
                cleanup_results["unused_imports"][str(file_path)] = unused
                cleanup_results["summary"]["total_unused_imports"] += len(unused)
                
        except Exception as e:
            logger.error(f"æ¸…ç†åˆ†æå¤±è´¥ {file_path}: {e}")
    
    return cleanup_results


def _generate_markdown_report(analysis_results: Dict[str, Any]) -> str:
    """ç”Ÿæˆè¯¦ç»†çš„Markdownæ ¼å¼æŠ¥å‘Š"""
    report_lines = []

    # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
    report_lines.append("# ğŸ“‹ ä»£ç è´¨é‡æ‰«ææŠ¥å‘Š")
    report_lines.append("")

    scan_info = analysis_results.get("scan_info", {})
    report_lines.append(f"**ğŸ“ æ‰«æè·¯å¾„**: {scan_info.get('path', 'N/A')}")
    report_lines.append(f"**â° æ‰«ææ—¶é—´**: {scan_info.get('timestamp', 'N/A')}")
    report_lines.append(f"**ğŸ” æ‰«æç±»å‹**: {', '.join(scan_info.get('scan_types', []))}")
    report_lines.append(f"**ğŸ“„ åˆ†ææ–‡ä»¶æ•°**: {scan_info.get('files_count', 0)}")

    # æ˜¾ç¤ºåˆ†æçš„æ–‡ä»¶åˆ—è¡¨
    files_analyzed = analysis_results.get("files_analyzed", [])
    if files_analyzed:
        report_lines.append("")
        report_lines.append("**ğŸ“‚ åˆ†ææ–‡ä»¶åˆ—è¡¨**:")
        for i, file_path in enumerate(files_analyzed, 1):
            file_name = Path(file_path).name
            report_lines.append(f"  {i}. `{file_name}`")

    report_lines.append("")
    report_lines.append("---")
    report_lines.append("")

    details = analysis_results.get("details", {})

    # æ‰«ææ€»ç»“
    report_lines.append("## ğŸ“Š æ‰«ææ€»ç»“")
    report_lines.append("")

    total_issues = 0
    if "style" in details:
        total_issues += details["style"]["summary"].get("total_issues", 0)
    if "security" in details:
        total_issues += details["security"]["summary"].get("total_issues", 0)

    report_lines.append(f"- **æ€»é—®é¢˜æ•°**: {total_issues}")

    if "security" in details:
        high_severity = details["security"]["summary"].get("high_severity", 0)
        report_lines.append(f"- **ä¸¥é‡é—®é¢˜æ•°**: {high_severity}")

    report_lines.append("")
    report_lines.append("---")
    report_lines.append("")
    
    # å¤æ‚åº¦åˆ†æ
    if "complexity" in details:
        complexity = details["complexity"]
        report_lines.append("## ğŸ” å¤æ‚åº¦åˆ†æ")
        report_lines.append("")
        report_lines.append(f"- **æ€»å‡½æ•°æ•°**: {complexity['summary'].get('total_functions', 0)}")
        report_lines.append(f"- **å¹³å‡å¤æ‚åº¦**: {complexity['summary'].get('average_complexity', 0)}")
        report_lines.append(f"- **é«˜å¤æ‚åº¦å‡½æ•°æ•°**: {len(complexity['summary'].get('high_complexity_functions', []))}")

        # è¯¦ç»†çš„å¤æ‚åº¦ä¿¡æ¯
        cyclomatic_complexity = complexity.get("cyclomatic_complexity", {})
        if cyclomatic_complexity:
            report_lines.append("")
            report_lines.append("### ğŸ“ˆ å„æ–‡ä»¶å¤æ‚åº¦è¯¦æƒ…")
            report_lines.append("")

            for file_path, functions in cyclomatic_complexity.items():
                if functions:  # åªæ˜¾ç¤ºæœ‰å‡½æ•°çš„æ–‡ä»¶
                    file_name = Path(file_path).name
                    report_lines.append(f"#### ğŸ“„ `{file_name}`")
                    report_lines.append("")

                    for func in functions:
                        complexity_level = "ğŸŸ¢" if func["complexity"] <= 5 else "ğŸŸ¡" if func["complexity"] <= 10 else "ğŸ”´"
                        report_lines.append(f"- **{func['name']}** (ç¬¬{func['lineno']}è¡Œ)")
                        report_lines.append(f"  - å¤æ‚åº¦: {complexity_level} {func['complexity']}")
                        report_lines.append(f"  - ç±»å‹: {func['type']}")
                        if func["complexity"] > 10:
                            report_lines.append(f"  - âš ï¸ **å»ºè®®é‡æ„**: å¤æ‚åº¦è¿‡é«˜ï¼Œå»ºè®®æ‹†åˆ†å‡½æ•°")
                        report_lines.append("")

        # é«˜å¤æ‚åº¦å‡½æ•°è­¦å‘Š
        high_complexity_functions = complexity['summary'].get('high_complexity_functions', [])
        if high_complexity_functions:
            report_lines.append("### ğŸš¨ é«˜å¤æ‚åº¦å‡½æ•°è­¦å‘Š")
            report_lines.append("")
            for func in high_complexity_functions:
                file_name = Path(func['file']).name
                report_lines.append(f"- **{func['function']}** åœ¨ `{file_name}` (å¤æ‚åº¦: {func['complexity']})")
            report_lines.append("")

        report_lines.append("---")
        report_lines.append("")
    
    # ä»£ç é£æ ¼æ£€æŸ¥
    if "style" in details:
        style = details["style"]
        report_lines.append("## ğŸ“ ä»£ç é£æ ¼æ£€æŸ¥")
        report_lines.append("")
        report_lines.append(f"- **æ€»é—®é¢˜æ•°**: {style['summary'].get('total_issues', 0)}")
        report_lines.append(f"- **æœ‰é—®é¢˜çš„æ–‡ä»¶æ•°**: {style['summary'].get('files_with_issues', 0)}")

        # è¯¦ç»†çš„é£æ ¼é—®é¢˜
        basic_checks = style.get("basic_checks", {})
        if basic_checks:
            report_lines.append("")
            report_lines.append("### ğŸ“ å„æ–‡ä»¶é£æ ¼é—®é¢˜è¯¦æƒ…")
            report_lines.append("")

            for file_path, issues in basic_checks.items():
                file_name = Path(file_path).name
                report_lines.append(f"#### ğŸ“„ `{file_name}` ({len(issues)}ä¸ªé—®é¢˜)")
                report_lines.append("")

                # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„
                issue_groups = {}
                for issue in issues:
                    issue_type = issue["type"]
                    if issue_type not in issue_groups:
                        issue_groups[issue_type] = []
                    issue_groups[issue_type].append(issue)

                for issue_type, type_issues in issue_groups.items():
                    type_name = {
                        "line_too_long": "ğŸ”¸ è¡Œé•¿åº¦è¶…é™",
                        "trailing_whitespace": "ğŸ”¸ å°¾éšç©ºæ ¼"
                    }.get(issue_type, f"ğŸ”¸ {issue_type}")

                    report_lines.append(f"**{type_name}** ({len(type_issues)}å¤„):")
                    for issue in type_issues[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                        report_lines.append(f"  - ç¬¬{issue['line']}è¡Œ: {issue['message']}")

                    if len(type_issues) > 5:
                        report_lines.append(f"  - ... è¿˜æœ‰{len(type_issues) - 5}ä¸ªç±»ä¼¼é—®é¢˜")
                    report_lines.append("")

        report_lines.append("---")
        report_lines.append("")
    
    # å®‰å…¨æ‰«æ
    if "security" in details:
        security = details["security"]
        report_lines.append("## ğŸ”’ å®‰å…¨æ‰«æ")
        report_lines.append("")
        report_lines.append(f"- **æ€»é—®é¢˜æ•°**: {security['summary'].get('total_issues', 0)}")
        report_lines.append(f"- **é«˜å±é—®é¢˜**: {security['summary'].get('high_severity', 0)}")
        report_lines.append(f"- **ä¸­å±é—®é¢˜**: {security['summary'].get('medium_severity', 0)}")
        report_lines.append(f"- **ä½å±é—®é¢˜**: {security['summary'].get('low_severity', 0)}")

        # è¯¦ç»†çš„å®‰å…¨é—®é¢˜
        security_checks = security.get("basic_security_checks", {})
        if security_checks:
            report_lines.append("")
            report_lines.append("### ğŸš¨ å®‰å…¨é—®é¢˜è¯¦æƒ…")
            report_lines.append("")

            for file_path, issues in security_checks.items():
                file_name = Path(file_path).name
                report_lines.append(f"#### ğŸ“„ `{file_name}` ({len(issues)}ä¸ªå®‰å…¨é—®é¢˜)")
                report_lines.append("")

                # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
                severity_groups = {"high": [], "medium": [], "low": []}
                for issue in issues:
                    severity = issue.get("severity", "low")
                    if severity in severity_groups:
                        severity_groups[severity].append(issue)

                # æ˜¾ç¤ºé«˜å±é—®é¢˜
                if severity_groups["high"]:
                    report_lines.append("**ğŸ”´ é«˜å±é—®é¢˜**:")
                    for issue in severity_groups["high"]:
                        report_lines.append(f"  - ç¬¬{issue['line']}è¡Œ: **{issue['message']}**")
                        report_lines.append(f"    - é—®é¢˜ç±»å‹: {issue['type']}")
                        report_lines.append(f"    - ğŸš¨ **ç«‹å³ä¿®å¤**: è¿™æ˜¯ä¸¥é‡çš„å®‰å…¨éšæ‚£")
                    report_lines.append("")

                # æ˜¾ç¤ºä¸­å±é—®é¢˜
                if severity_groups["medium"]:
                    report_lines.append("**ğŸŸ¡ ä¸­å±é—®é¢˜**:")
                    for issue in severity_groups["medium"]:
                        report_lines.append(f"  - ç¬¬{issue['line']}è¡Œ: {issue['message']}")
                        report_lines.append(f"    - é—®é¢˜ç±»å‹: {issue['type']}")
                        report_lines.append(f"    - âš ï¸ **å»ºè®®ä¿®å¤**: å­˜åœ¨æ½œåœ¨å®‰å…¨é£é™©")
                    report_lines.append("")

                # æ˜¾ç¤ºä½å±é—®é¢˜
                if severity_groups["low"]:
                    report_lines.append("**ğŸŸ¢ ä½å±é—®é¢˜**:")
                    for issue in severity_groups["low"]:
                        report_lines.append(f"  - ç¬¬{issue['line']}è¡Œ: {issue['message']}")
                        report_lines.append(f"    - é—®é¢˜ç±»å‹: {issue['type']}")
                    report_lines.append("")

        report_lines.append("---")
        report_lines.append("")
    
    # æ–‡æ¡£è´¨é‡
    if "documentation" in details:
        doc = details["documentation"]
        report_lines.append("## ğŸ“š æ–‡æ¡£è´¨é‡")
        report_lines.append("")
        report_lines.append(f"- **æ€»å‡½æ•°æ•°**: {doc['summary'].get('total_functions', 0)}")
        report_lines.append(f"- **æœ‰æ–‡æ¡£çš„å‡½æ•°æ•°**: {doc['summary'].get('documented_functions', 0)}")
        report_lines.append(f"- **æ–‡æ¡£è¦†ç›–ç‡**: {doc['summary'].get('coverage_percentage', 0)}%")

        # è¯¦ç»†çš„æ–‡æ¡£ä¿¡æ¯
        docstring_coverage = doc.get("docstring_coverage", {})
        if docstring_coverage:
            report_lines.append("")
            report_lines.append("### ğŸ“– å„æ–‡ä»¶æ–‡æ¡£è¯¦æƒ…")
            report_lines.append("")

            for file_path, functions in docstring_coverage.items():
                file_name = Path(file_path).name
                documented_count = sum(1 for func in functions if func["has_docstring"])
                total_count = len(functions)
                coverage = (documented_count / total_count * 100) if total_count > 0 else 0

                report_lines.append(f"#### ğŸ“„ `{file_name}` (è¦†ç›–ç‡: {coverage:.1f}%)")
                report_lines.append("")

                # æœ‰æ–‡æ¡£çš„å‡½æ•°
                documented_functions = [f for f in functions if f["has_docstring"]]
                if documented_functions:
                    report_lines.append("**âœ… æœ‰æ–‡æ¡£çš„å‡½æ•°**:")
                    for func in documented_functions:
                        report_lines.append(f"  - `{func['name']}` (ç¬¬{func['line']}è¡Œ)")
                    report_lines.append("")

                # ç¼ºå°‘æ–‡æ¡£çš„å‡½æ•°
                undocumented_functions = [f for f in functions if not f["has_docstring"]]
                if undocumented_functions:
                    report_lines.append("**âŒ ç¼ºå°‘æ–‡æ¡£çš„å‡½æ•°**:")
                    for func in undocumented_functions:
                        report_lines.append(f"  - `{func['name']}` (ç¬¬{func['line']}è¡Œ)")
                        report_lines.append(f"    - ğŸ’¡ **å»ºè®®**: æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜å‡½æ•°ç”¨é€”ã€å‚æ•°å’Œè¿”å›å€¼")
                    report_lines.append("")

        # æ–‡æ¡£è´¨é‡å»ºè®®
        coverage_percentage = doc['summary'].get('coverage_percentage', 0)
        if coverage_percentage < 50:
            report_lines.append("### ğŸ“ æ–‡æ¡£æ”¹è¿›å»ºè®®")
            report_lines.append("")
            report_lines.append("- ğŸ”´ **æ–‡æ¡£è¦†ç›–ç‡åä½**: å»ºè®®ä¸ºæ‰€æœ‰å…¬å…±å‡½æ•°æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²")
            report_lines.append("- ğŸ“‹ **æ–‡æ¡£è§„èŒƒ**: å»ºè®®éµå¾ªPEP 257æ–‡æ¡£å­—ç¬¦ä¸²çº¦å®š")
            report_lines.append("- ğŸ¯ **ç›®æ ‡**: å°†æ–‡æ¡£è¦†ç›–ç‡æå‡è‡³80%ä»¥ä¸Š")
            report_lines.append("")
        elif coverage_percentage < 80:
            report_lines.append("### ğŸ“ æ–‡æ¡£æ”¹è¿›å»ºè®®")
            report_lines.append("")
            report_lines.append("- ğŸŸ¡ **æ–‡æ¡£è¦†ç›–ç‡è‰¯å¥½**: ç»§ç»­ä¸ºå‰©ä½™å‡½æ•°æ·»åŠ æ–‡æ¡£")
            report_lines.append("- ğŸ¯ **ç›®æ ‡**: å°†æ–‡æ¡£è¦†ç›–ç‡æå‡è‡³80%ä»¥ä¸Š")
            report_lines.append("")

        report_lines.append("---")
        report_lines.append("")
    
    # ä»£ç æ¸…ç†å»ºè®®
    if "cleanup" in details:
        cleanup = details["cleanup"]
        report_lines.append("## ğŸ§¹ ä»£ç æ¸…ç†å»ºè®®")
        report_lines.append("")
        report_lines.append(f"- **æœªä½¿ç”¨å¯¼å…¥æ€»æ•°**: {cleanup['summary'].get('total_unused_imports', 0)}")

        # è¯¦ç»†çš„æ¸…ç†ä¿¡æ¯
        unused_imports = cleanup.get("unused_imports", {})
        if unused_imports:
            report_lines.append("")
            report_lines.append("### ğŸ“¦ æœªä½¿ç”¨å¯¼å…¥è¯¦æƒ…")
            report_lines.append("")

            for file_path, imports in unused_imports.items():
                file_name = Path(file_path).name
                report_lines.append(f"#### ğŸ“„ `{file_name}` ({len(imports)}ä¸ªæœªä½¿ç”¨å¯¼å…¥)")
                report_lines.append("")

                for import_name in imports:
                    report_lines.append(f"  - `{import_name}`")
                    report_lines.append(f"    - ğŸ’¡ **å»ºè®®**: ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥ä»¥å‡å°‘ä»£ç å†—ä½™")
                report_lines.append("")

        report_lines.append("---")
        report_lines.append("")

    # æ”¹è¿›å»ºè®®
    report_lines.append("## ğŸ’¡ ç»¼åˆæ”¹è¿›å»ºè®®")
    report_lines.append("")

    # æ ¹æ®åˆ†æç»“æœç”Ÿæˆä¼˜å…ˆçº§å»ºè®®
    high_priority = []
    medium_priority = []
    low_priority = []

    # é«˜ä¼˜å…ˆçº§é—®é¢˜
    if "security" in details and details["security"]["summary"].get("high_severity", 0) > 0:
        high_priority.append(f"ä¿®å¤ {details['security']['summary']['high_severity']} ä¸ªé«˜å±å®‰å…¨é—®é¢˜")

    if "complexity" in details and details["complexity"]["summary"].get("high_complexity_functions"):
        high_count = len(details["complexity"]["summary"]["high_complexity_functions"])
        high_priority.append(f"é‡æ„ {high_count} ä¸ªé«˜å¤æ‚åº¦å‡½æ•°")

    # ä¸­ä¼˜å…ˆçº§é—®é¢˜
    if "security" in details and details["security"]["summary"].get("medium_severity", 0) > 0:
        medium_priority.append(f"ä¿®å¤ {details['security']['summary']['medium_severity']} ä¸ªä¸­å±å®‰å…¨é—®é¢˜")

    if "documentation" in details and details["documentation"]["summary"].get("coverage_percentage", 0) < 80:
        coverage = details["documentation"]["summary"]["coverage_percentage"]
        undocumented = details["documentation"]["summary"]["total_functions"] - details["documentation"]["summary"]["documented_functions"]
        medium_priority.append(f"ä¸º {undocumented} ä¸ªå‡½æ•°æ·»åŠ æ–‡æ¡£ (å½“å‰è¦†ç›–ç‡: {coverage}%)")

    # ä½ä¼˜å…ˆçº§é—®é¢˜
    if "style" in details and details["style"]["summary"].get("total_issues", 0) > 0:
        style_issues = details["style"]["summary"]["total_issues"]
        low_priority.append(f"ä¿®å¤ {style_issues} ä¸ªä»£ç é£æ ¼é—®é¢˜")

    if "cleanup" in details and details["cleanup"]["summary"].get("total_unused_imports", 0) > 0:
        unused_count = details["cleanup"]["summary"]["total_unused_imports"]
        low_priority.append(f"æ¸…ç† {unused_count} ä¸ªæœªä½¿ç”¨çš„å¯¼å…¥")

    # æ˜¾ç¤ºå»ºè®®
    if high_priority:
        report_lines.append("### ğŸ”´ é«˜ä¼˜å…ˆçº§ (ç«‹å³å¤„ç†)")
        for suggestion in high_priority:
            report_lines.append(f"1. **{suggestion}**")
            report_lines.append("   - å½±å“: å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§")
            report_lines.append("   - å»ºè®®: ç«‹å³ä¿®å¤")
        report_lines.append("")

    if medium_priority:
        report_lines.append("### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ (è¿‘æœŸå¤„ç†)")
        for suggestion in medium_priority:
            report_lines.append(f"2. **{suggestion}**")
            report_lines.append("   - å½±å“: ä»£ç è´¨é‡å’Œå¯è¯»æ€§")
            report_lines.append("   - å»ºè®®: 1-2å‘¨å†…å¤„ç†")
        report_lines.append("")

    if low_priority:
        report_lines.append("### ğŸŸ¢ ä½ä¼˜å…ˆçº§ (æœ‰æ—¶é—´æ—¶å¤„ç†)")
        for suggestion in low_priority:
            report_lines.append(f"3. **{suggestion}**")
            report_lines.append("   - å½±å“: ä»£ç è§„èŒƒå’Œæ•´æ´åº¦")
            report_lines.append("   - å»ºè®®: æœ‰æ—¶é—´æ—¶å¤„ç†")
        report_lines.append("")

    if not (high_priority or medium_priority or low_priority):
        report_lines.append("### âœ… ä»£ç è´¨é‡ä¼˜ç§€")
        report_lines.append("æœªå‘ç°éœ€è¦æ”¹è¿›çš„é—®é¢˜ï¼Œä»£ç è´¨é‡è‰¯å¥½ï¼")
        report_lines.append("")

    # æ€»ä½“è¯„ä¼°
    report_lines.append("### ğŸ“ˆ æ€»ä½“è¯„ä¼°")
    report_lines.append("")

    # è®¡ç®—è´¨é‡åˆ†æ•°
    security_score = 100
    if "security" in details:
        high_issues = details["security"]["summary"].get("high_severity", 0)
        medium_issues = details["security"]["summary"].get("medium_severity", 0)
        security_score = max(0, 100 - (high_issues * 30) - (medium_issues * 10))

    complexity_score = 100
    if "complexity" in details:
        high_complexity = len(details["complexity"]["summary"].get("high_complexity_functions", []))
        avg_complexity = details["complexity"]["summary"].get("average_complexity", 0)
        complexity_score = max(0, 100 - (high_complexity * 20) - max(0, (avg_complexity - 5) * 5))

    doc_score = details.get("documentation", {}).get("summary", {}).get("coverage_percentage", 100)

    overall_score = (security_score + complexity_score + doc_score) / 3

    if overall_score >= 90:
        grade = "ğŸŸ¢ ä¼˜ç§€"
        conclusion = "ä»£ç è´¨é‡ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒï¼"
    elif overall_score >= 75:
        grade = "ğŸŸ¡ è‰¯å¥½"
        conclusion = "ä»£ç è´¨é‡è‰¯å¥½ï¼Œæœ‰å°‘é‡æ”¹è¿›ç©ºé—´ã€‚"
    elif overall_score >= 60:
        grade = "ğŸŸ  ä¸€èˆ¬"
        conclusion = "ä»£ç è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æŒ‰ä¼˜å…ˆçº§é€æ­¥æ”¹è¿›ã€‚"
    else:
        grade = "ğŸ”´ éœ€è¦æ”¹è¿›"
        conclusion = "ä»£ç è´¨é‡æœ‰å¾…æé«˜ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨é«˜ä¼˜å…ˆçº§é—®é¢˜ã€‚"

    report_lines.append(f"**ä»£ç è´¨é‡ç­‰çº§**: {grade}")
    report_lines.append(f"**ç»¼åˆè¯„åˆ†**: {overall_score:.1f}/100")
    report_lines.append(f"**ç»“è®º**: {conclusion}")
    report_lines.append("")

    # è¯„åˆ†è¯¦æƒ…
    report_lines.append("**è¯„åˆ†è¯¦æƒ…**:")
    report_lines.append(f"- å®‰å…¨æ€§: {security_score:.1f}/100")
    report_lines.append(f"- å¤æ‚åº¦: {complexity_score:.1f}/100")
    report_lines.append(f"- æ–‡æ¡£æ€§: {doc_score:.1f}/100")
    report_lines.append("")

    report_lines.append("---")
    report_lines.append("")
    report_lines.append("*ğŸ“Š æŠ¥å‘Šç”± AutoGen ä»£ç æ‰«æå·¥å…·ç”Ÿæˆ*")
    report_lines.append(f"*ğŸ•’ ç”Ÿæˆæ—¶é—´: {scan_info.get('timestamp', 'N/A')}*")

    return "\n".join(report_lines)
